{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression for MNIST Digit Classification\nIn this notebook we will simply use logistic regression to classify the MNIST digits. No CNN and no fancy stuff. Let´s see how far we can get with this."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torchvision","execution_count":385,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Data Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ndisplay(df)","execution_count":386,"outputs":[{"output_type":"display_data","data":{"text/plain":"       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0          1       0       0       0       0       0       0       0       0   \n1          0       0       0       0       0       0       0       0       0   \n2          1       0       0       0       0       0       0       0       0   \n3          4       0       0       0       0       0       0       0       0   \n4          0       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n41995      0       0       0       0       0       0       0       0       0   \n41996      1       0       0       0       0       0       0       0       0   \n41997      7       0       0       0       0       0       0       0       0   \n41998      6       0       0       0       0       0       0       0       0   \n41999      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0           0  ...         0         0         0         0         0   \n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n41995       0  ...         0         0         0         0         0   \n41996       0  ...         0         0         0         0         0   \n41997       0  ...         0         0         0         0         0   \n41998       0  ...         0         0         0         0         0   \n41999       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n41995         0         0         0         0         0  \n41996         0         0         0         0         0  \n41997         0         0         0         0         0  \n41998         0         0         0         0         0  \n41999         0         0         0         0         0  \n\n[42000 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41996</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41997</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41998</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41999</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>42000 rows × 785 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning\nNo Data Cleaning apllied here."},{"metadata":{},"cell_type":"markdown","source":"### Data Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = df.to_numpy().astype(np.float32)\narr","execution_count":387,"outputs":[{"output_type":"execute_result","execution_count":387,"data":{"text/plain":"array([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [7., 0., 0., ..., 0., 0., 0.],\n       [6., 0., 0., ..., 0., 0., 0.],\n       [9., 0., 0., ..., 0., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = torch.from_numpy(arr)\ndataset","execution_count":388,"outputs":[{"output_type":"execute_result","execution_count":388,"data":{"text/plain":"tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [1., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [7., 0., 0.,  ..., 0., 0., 0.],\n        [6., 0., 0.,  ..., 0., 0., 0.],\n        [9., 0., 0.,  ..., 0., 0., 0.]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Split inputs (images) and outputs (labels) to tuples with TensorDataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset\n\nlabels = []\nfor i in range(len(dataset)):\n    labels.append(int(dataset[i][0]))\nlabels = torch.Tensor(labels).type(torch.LongTensor)\n\nimages = dataset.narrow(1, 1, 784)\n\nprint(images)\nprint(labels)","execution_count":389,"outputs":[{"output_type":"stream","text":"tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\ntensor([1, 0, 1,  ..., 7, 6, 9])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = TensorDataset(images, labels)\ndataset","execution_count":390,"outputs":[{"output_type":"execute_result","execution_count":390,"data":{"text/plain":"<torch.utils.data.dataset.TensorDataset at 0x7f5f36bf9050>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[0]","execution_count":391,"outputs":[{"output_type":"execute_result","execution_count":391,"data":{"text/plain":"(tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         188., 255.,  94.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0., 191., 250., 253.,  93.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0., 123., 248., 253., 167.,  10.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 247., 253.,\n         208.,  13.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          29., 207., 253., 235.,  77.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,  54., 209., 253., 253.,  88.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,  93., 254., 253., 238., 170.,  17.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23., 210., 254.,\n         253., 159.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          16., 209., 253., 254., 240.,  81.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,  27., 253., 253., 254.,  13.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,  20., 206., 254., 254., 198.,\n           7.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 168.,\n         253., 253., 196.,   7.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,  20., 203., 253., 248.,  76.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,  22., 188., 253., 245.,  93.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 103., 253., 253.,\n         191.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          89., 240., 253., 195.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,  15., 220., 253., 253.,  80.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,  94., 253., 253., 253.,  94.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  89.,\n         251., 253., 250., 131.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0., 214., 218.,  95.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.]),\n tensor(1))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgID = 568\n\nlabel = labels[imgID]\nprint(label)\n\nimage = torch.reshape(images[imgID], (28, 28))\nprint(image)","execution_count":392,"outputs":[{"output_type":"stream","text":"tensor(3)\ntensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,  38., 122., 153.,\n         255., 255., 254., 255., 255., 243., 119.,   7.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  25., 176., 253., 253., 253.,\n         253., 253., 253., 253., 253., 253., 253., 196.,  36.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,  54., 178., 232., 253., 250., 165., 139.,\n          71.,  32.,  32.,  32., 120., 151., 249., 253., 236., 115.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   7., 123., 241., 253., 221., 114.,  39.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,  27., 192., 253., 225.,  41.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,  73., 253., 253., 140.,  17.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253., 145.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0., 146., 253., 249.,  15.,  35.,  12.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,  54., 205., 253., 140.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0., 126., 253., 253., 177.,  34.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,  62., 237., 253., 240.,  30.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,  25., 167., 243., 253., 135.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0., 141., 235., 253., 242.,  75.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,  19.,  21.,   6.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,  11., 146., 253., 253., 247., 115.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          12., 140., 162., 223., 253., 253., 240.,  73.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          55., 253., 253., 253., 253., 232.,  40.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          27., 124., 225., 237., 253., 240.,  57.,   4.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,  21., 150., 253., 253.,  68.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,  19., 117., 253., 234.,  92.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,  13., 199., 253., 170.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,  58., 138.,   7.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,  93., 253., 221.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0., 151., 253., 115.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,  23., 240., 253., 177.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0., 138., 253., 253., 249., 249., 196.,\n         141., 141., 141., 166., 207., 245., 253., 253.,  99.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,  13., 110., 216., 253., 253., 253.,\n         253., 253., 253., 253., 253., 253., 235.,  93.,   2.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 105., 145., 191.,\n         168., 253., 151., 145., 145.,  88.,  28.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.],\n        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.imshow(image, cmap='gray')    # Choose imgID in last cell, to visualize here","execution_count":393,"outputs":[{"output_type":"execute_result","execution_count":393,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f5f3fde4990>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"### Split Training Data into Train & Validation Set\nIn this competition 28000 images are already assigned to the **Test set**. From the 42000 images given as the **Training set** we will have to pick a small portion for **Validation**. Let´s pick 4000 for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import random_split\n\ntrain_ds, val_ds = random_split(dataset, [38000, 4000])\nlen(train_ds), len(val_ds)","execution_count":394,"outputs":[{"output_type":"execute_result","execution_count":394,"data":{"text/plain":"(38000, 4000)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Databatch Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 100\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)\nval_dl = DataLoader(val_ds, len(val_ds))    # Normally you DON'T choose batchsize=datasize! \n                                            # Normally you split into more than 1 batches.\n                                            # So, don't try this at home lol","execution_count":395,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for xb, yb in train_dl:    # Test batches of train_dl or val_dl\n    print(xb)\n    print(yb)\n    break","execution_count":396,"outputs":[{"output_type":"stream","text":"tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\ntensor([7, 9, 0, 6, 8, 3, 3, 3, 3, 7, 1, 8, 0, 5, 6, 1, 8, 1, 9, 4, 5, 0, 8, 8,\n        0, 0, 7, 4, 5, 9, 4, 5, 8, 3, 1, 8, 1, 3, 7, 0, 7, 9, 5, 8, 9, 7, 9, 5,\n        1, 5, 8, 9, 5, 6, 4, 2, 8, 1, 9, 9, 6, 1, 6, 9, 0, 5, 8, 9, 6, 3, 8, 9,\n        0, 1, 2, 4, 2, 4, 9, 9, 5, 0, 8, 5, 0, 4, 2, 6, 3, 2, 1, 4, 6, 0, 1, 9,\n        2, 1, 0, 6])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\ninput_size = 784    # 28*28\nnum_classes = 10    # {0,1,...,9}\n\nmodel = nn.Linear(input_size, num_classes)","execution_count":397,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.weight.shape)\nprint(model.weight)","execution_count":398,"outputs":[{"output_type":"stream","text":"torch.Size([10, 784])\nParameter containing:\ntensor([[-0.0088, -0.0099, -0.0008,  ..., -0.0042,  0.0068, -0.0136],\n        [ 0.0334,  0.0277,  0.0018,  ..., -0.0103,  0.0172, -0.0151],\n        [ 0.0161, -0.0120,  0.0211,  ..., -0.0338, -0.0293, -0.0299],\n        ...,\n        [ 0.0340,  0.0140,  0.0255,  ..., -0.0096, -0.0058,  0.0083],\n        [ 0.0193,  0.0348,  0.0226,  ..., -0.0104,  0.0021,  0.0263],\n        [-0.0300,  0.0210, -0.0176,  ...,  0.0132, -0.0177, -0.0317]],\n       requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.bias.shape)\nprint(model.bias)","execution_count":399,"outputs":[{"output_type":"stream","text":"torch.Size([10])\nParameter containing:\ntensor([ 0.0089, -0.0011,  0.0232,  0.0144,  0.0131,  0.0048, -0.0102, -0.0017,\n         0.0107, -0.0036], requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for xb, yb in train_dl:\n    print(xb)\n    print(yb)\n    break","execution_count":400,"outputs":[{"output_type":"stream","text":"tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\ntensor([0, 4, 1, 2, 8, 5, 9, 9, 8, 1, 6, 0, 1, 7, 2, 5, 9, 3, 0, 0, 1, 0, 9, 2,\n        1, 9, 0, 8, 2, 3, 4, 5, 8, 5, 5, 9, 8, 3, 6, 1, 4, 2, 8, 5, 2, 6, 7, 6,\n        9, 7, 8, 6, 5, 1, 1, 5, 6, 0, 2, 9, 9, 1, 4, 4, 4, 1, 6, 0, 2, 6, 1, 3,\n        5, 3, 9, 2, 7, 6, 8, 0, 3, 1, 2, 5, 7, 1, 9, 0, 9, 9, 5, 1, 7, 8, 1, 5,\n        0, 3, 6, 7])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for images, labels in train_dl:\n    outputs = model(images)\n    break\n\nprint('batch 1:')\nprint('outputs.shape : ', outputs.shape)\nprint('Outputs:\\n', outputs.data)","execution_count":401,"outputs":[{"output_type":"stream","text":"batch 1:\noutputs.shape :  torch.Size([100, 10])\nOutputs:\n tensor([[ 3.0583e+01,  6.3276e+01, -5.9252e+01, -4.0008e+01, -5.6348e+01,\n          2.1429e+01,  1.8356e+01,  2.1566e+01, -1.2454e+01,  6.1978e+01],\n        [ 1.7372e+01, -6.6730e+01,  1.7291e+01, -5.4290e+01, -8.8736e+01,\n          3.8537e+01, -1.3398e+01, -7.7263e+01, -4.2544e+00, -2.1078e+01],\n        [ 7.5221e+01, -8.7835e+00,  2.5598e+00,  6.8949e+01, -1.8451e+01,\n          3.9535e+01, -3.2283e+01,  1.4208e+01,  1.4888e+01,  9.2762e+00],\n        [ 1.0720e+02,  7.2937e+01, -1.8735e+01, -1.6252e+01, -1.6460e+02,\n         -3.4076e+01,  5.2533e+01,  5.4625e+00, -4.2064e+00,  5.3450e+01],\n        [ 1.1970e+01,  1.2765e+01, -2.2987e+01, -4.6174e+01, -7.1285e-01,\n          1.0936e+01, -2.7630e+01, -3.2631e+01, -2.7598e+01, -2.3636e+01],\n        [ 2.6443e+00,  3.8717e+01, -6.8147e+01,  3.2753e+01, -1.7070e+01,\n          7.1773e+01, -6.5645e-01, -5.9131e+00, -1.1775e+01, -5.0533e+01],\n        [ 6.6894e+00,  5.6143e+01,  8.0827e+00, -2.2631e+01, -8.4099e+01,\n          5.4063e+01,  5.6998e+01, -1.7856e+01, -2.4149e+01,  6.4679e+01],\n        [-2.7190e+01,  7.7275e+00, -7.5025e+00, -7.2193e+01, -1.4046e+01,\n          1.4108e+01,  1.8285e+00, -4.9805e+01,  3.0761e+00,  7.6367e+01],\n        [ 1.4325e+02, -3.0177e+01,  1.7836e+01,  1.6203e+00,  3.9640e+00,\n         -1.0765e+01, -4.2137e+01,  1.1314e+02, -9.0205e+00,  1.3794e+01],\n        [ 1.8305e+01, -6.8689e+01, -1.1774e+01,  6.4011e+01, -6.5017e+01,\n         -5.1342e+00,  1.9400e+01,  2.4078e+01, -6.4779e+01, -2.7543e+01],\n        [-1.4380e+01,  2.1320e+00,  1.7833e+01,  3.3879e+01, -3.3102e+01,\n          8.1878e+01,  2.7065e+01,  2.2846e+01, -1.4451e+01,  3.7873e+01],\n        [ 5.6217e+00, -5.9877e-01, -8.9956e+01,  2.2255e+00,  9.5991e+00,\n          7.7019e+00, -3.8379e+01, -4.9071e+01,  3.5542e+01, -5.8004e+00],\n        [ 8.7993e+01, -2.7920e+01, -1.6478e+01, -4.4630e+01, -9.4381e+00,\n         -5.0664e+00, -7.1857e+01,  8.1020e+00, -1.0054e+01, -1.6700e+01],\n        [ 7.6505e+01,  5.0587e+00, -1.1396e+02, -1.2026e+02,  4.1442e+01,\n          6.5505e+01,  8.7271e+01,  2.9242e+01, -6.0313e+01, -8.3410e+00],\n        [ 3.0165e+01, -5.6723e+00, -9.5246e+01, -1.6568e+01, -7.6490e+01,\n          5.4288e+01, -7.9382e+01, -4.8747e+01, -1.8426e+01, -1.9155e+01],\n        [ 4.7595e+01, -2.2680e+01,  5.5964e+00,  2.2753e+01, -7.6199e+01,\n         -8.9807e+00,  6.2029e+00, -7.1380e+01, -1.7598e+01, -2.4361e+01],\n        [ 9.0610e+01,  2.9109e+01, -2.5578e+01, -7.4584e+00, -1.4481e+02,\n          9.9473e+00,  3.7770e+01, -5.8111e+01,  7.1250e+00, -1.2068e+01],\n        [ 2.5256e-01,  1.7753e+01, -5.5353e+01, -5.1331e+01, -1.3569e+01,\n          5.5780e+01, -1.9475e+01,  1.3338e+01,  1.2533e+01, -2.4716e+01],\n        [-3.2151e+00, -8.3566e+01, -7.9900e+00,  1.2951e-01, -1.1205e+02,\n          3.9766e+01, -2.4909e+01, -8.4957e+01, -2.9816e+01, -8.5106e-02],\n        [ 6.4247e+01, -3.3750e+01, -1.0972e+01, -1.9503e+01, -3.8971e+01,\n          3.4521e+01,  1.9572e+01,  9.0489e+00, -3.7635e+01, -5.9018e+01],\n        [ 6.9557e+01, -3.6224e+01, -4.2199e+01,  4.7449e+01, -2.8633e+01,\n          6.2258e+01,  1.1388e+01, -2.3193e+01, -2.9362e+01, -4.3577e+01],\n        [ 9.3682e+01,  6.2575e+01, -4.8994e+01, -3.0150e+01, -7.1708e+01,\n          8.1196e+00,  4.0686e+01,  4.8923e+01, -5.6994e+01,  2.8866e+01],\n        [ 7.2635e+01, -3.7669e+01,  8.5696e+00, -2.3677e+01, -2.9020e+01,\n          5.6806e+00, -1.8109e+01,  1.1336e+01, -5.1911e+01, -7.0748e+01],\n        [-1.7953e+01, -1.3866e+01, -2.0225e+01, -6.6053e+01, -3.0210e+01,\n          1.8972e+01, -2.0946e+01, -2.7796e+01, -1.0019e+01,  7.8546e+00],\n        [ 6.2955e+00,  2.2245e+01, -8.4614e+01, -1.1988e+02, -3.3891e+01,\n          2.6746e+01,  5.2974e+00,  3.1526e+01,  2.6075e+01,  2.3294e+00],\n        [ 6.5617e+01,  3.0159e+01,  3.7823e+01, -7.6191e-01,  1.8720e+01,\n         -2.4836e+01, -5.1348e+01,  4.4274e+01, -1.9702e+01, -1.1158e+00],\n        [ 2.4569e+01,  3.1727e+01, -1.8982e+01, -4.8605e+01,  1.9107e+01,\n         -1.0315e+00, -3.5842e+01, -1.2744e+01, -4.9878e+01, -6.7557e+01],\n        [ 6.9464e+01,  1.9040e+00,  1.1959e+00,  3.3340e+01, -2.7048e+01,\n          5.2516e+01, -4.5179e+01, -4.2021e-01, -3.2860e+01, -7.7486e+01],\n        [ 2.0130e+01,  1.0972e+00, -5.4956e+01,  2.9280e+01, -3.7172e+00,\n          3.8025e+01,  1.6165e+01, -1.3153e+01,  1.6666e+01, -4.2826e+01],\n        [ 3.2474e+01, -1.5934e+01, -1.8942e+01,  5.4003e+01,  1.3259e+01,\n          3.1902e+01, -1.7643e+01,  7.7950e+01, -2.5745e+01, -3.5985e+00],\n        [ 5.7761e+01, -2.9235e+01, -6.7288e+01,  1.8899e+01, -9.6648e+01,\n          7.6526e+01, -5.1403e+01, -9.0608e+00,  4.0929e+01, -1.1753e+01],\n        [ 1.6391e+01,  4.9866e+01, -2.5856e+01, -2.9801e+01, -2.7402e+01,\n          8.4194e+01,  2.1028e+01,  5.9956e+01, -2.6267e+01, -1.0909e+01],\n        [ 6.1225e+01,  3.9521e+01, -4.5284e+01, -3.1523e+01, -4.4731e+01,\n          2.9102e+01, -7.0478e+01,  3.0063e+01,  1.5381e+01, -4.3229e+01],\n        [ 5.0946e+01,  1.4602e+01,  2.0368e+00,  7.5226e+01, -1.3622e+01,\n          1.5026e+00,  1.9063e+00,  9.0494e+01,  9.0866e+00, -1.0709e+01],\n        [ 5.1329e+01,  4.8585e+01,  1.5431e+01,  3.1015e+01, -5.3365e+01,\n          7.2134e+01, -2.5872e+01,  4.9099e+01,  2.4252e+01, -5.4378e+01],\n        [-5.6726e-01, -3.1268e+01, -2.5366e+01, -8.9237e+00, -3.2516e+01,\n          5.1460e+01, -8.6295e+00,  3.0979e+01, -3.9539e+01, -6.6481e+01],\n        [-2.4483e+00, -5.2001e+01,  4.4807e-01,  2.6784e+01, -9.4078e+01,\n          3.9469e+01,  5.0677e+01, -9.4415e+01,  8.7804e+01,  4.9626e+01],\n        [ 6.1151e+01, -5.1369e+01,  8.3452e+00,  1.3099e+01, -1.2366e+02,\n         -2.3486e+01,  4.0958e+01, -5.6176e+01,  3.7446e+01, -2.2618e+01],\n        [ 4.5201e+01,  8.1499e+00, -6.7666e+01, -1.0661e+01, -6.8496e+01,\n          8.6609e+01, -5.2346e+01, -4.8776e+01,  3.1952e+00, -5.3902e+01],\n        [-3.3072e+01,  4.1981e+01, -4.7676e+01, -3.5368e+01,  1.8997e+01,\n         -7.3385e+00, -1.4379e+01, -1.1950e+01, -1.8221e+01, -1.0947e+01],\n        [ 8.2714e+00, -3.9990e+01, -5.2631e+01, -1.0056e+02, -1.9322e+01,\n          4.6142e+01,  1.9075e+01,  3.1072e+01, -1.7889e+01,  5.4665e+01],\n        [ 6.4640e+01, -2.3251e+00, -4.5697e+01, -1.6026e+01, -3.6914e-01,\n          1.5778e+01,  1.0493e+00, -6.5487e+00,  2.5851e+01, -6.2430e+00],\n        [ 9.5250e+01, -4.1354e+01, -1.7917e+01,  2.5965e+01, -9.1872e+01,\n          6.7859e+01, -6.2906e+01,  2.6466e+01, -7.8487e+00, -1.8881e+01],\n        [ 7.4960e+01,  8.6473e+00,  8.6010e+00,  1.8877e+01, -7.7930e+00,\n          5.5657e+01,  1.2797e+01, -1.8751e+00,  5.8287e+00,  4.6700e+01],\n        [-4.3841e+00,  4.2402e+01, -3.7305e+01, -9.0439e+01, -3.7403e+01,\n          4.6316e+00, -8.9109e+00, -6.2751e+01, -4.0162e+01, -4.3587e+01],\n        [-1.2667e+01, -1.7061e+01, -1.0264e+01, -2.2648e+01, -1.3008e+02,\n          2.9105e+00, -1.6837e+01, -4.9060e+01,  1.5842e+01,  6.0231e+00],\n        [ 5.4109e+01,  2.7839e+01, -9.0579e+01,  3.4398e+00, -3.6356e+01,\n          7.9494e+01,  2.0166e+01,  2.7092e+00, -2.0833e+00,  2.1900e+01],\n        [ 4.5818e+01, -6.9566e+01, -1.6313e+01,  8.3059e+00, -1.5167e+02,\n          1.9430e+01, -2.9500e+01, -5.5999e+01,  2.9242e+01, -4.0628e+01],\n        [ 5.5747e+01,  3.2893e+01, -2.9537e+01, -3.1064e+01, -9.3360e+01,\n          8.0120e+00,  1.3603e+01, -4.1948e+01,  4.6893e+01,  1.2515e+01],\n        [ 8.5947e+01,  1.4624e+01, -7.2792e+01,  3.3858e+01, -3.8635e+01,\n          4.5196e+01, -9.3053e+00, -8.4211e+00, -3.1071e+01, -1.7594e+01],\n        [ 3.7057e+01,  7.0209e+00, -1.7467e+01, -1.4533e+01, -2.0508e+01,\n          3.1115e+01, -3.7274e+00, -8.9532e+00, -3.7755e+01, -8.7532e+01],\n        [ 3.2933e+01,  4.9411e+01, -6.6726e+01, -4.2880e+01, -3.9782e+01,\n          4.6379e+01, -8.8570e+00,  4.5754e+01, -1.2092e+01, -4.9494e+01],\n        [ 2.2154e+01,  2.7700e+00, -1.6378e+01, -3.3084e+01, -4.7569e+01,\n          4.5292e+01,  2.3950e+01,  7.3493e+01, -4.1236e+01, -4.7973e+01],\n        [ 4.4843e+01, -3.9237e+01,  1.3117e+01,  1.1556e+01, -5.9410e+01,\n          3.3648e+01, -2.9357e+01, -2.9315e+01,  1.4459e+01,  8.1786e+00],\n        [ 2.2350e+01, -2.0809e+01, -1.7318e+01, -7.1135e+01,  2.1573e+01,\n          3.5977e+01, -3.0165e+00,  1.5653e+01, -5.3061e+01, -4.2245e+00],\n        [ 1.7130e+01,  1.2811e+01, -2.9229e+01,  2.2547e+01, -7.5719e+00,\n          2.9586e+01,  1.7728e+01,  3.3326e+01, -4.3418e+01,  7.0987e+01],\n        [-1.6915e+01,  9.4622e+00, -3.7152e+01, -2.0941e+01, -4.6307e+01,\n         -1.6355e+01,  2.6389e+01, -3.4859e+01,  2.7743e+01, -4.7394e+00],\n        [ 7.7594e+01,  3.4873e+01, -4.3130e+01, -7.1174e+01,  1.3070e+01,\n          4.0772e+00,  1.1768e+01,  5.3498e+01, -8.4196e+01,  9.1003e+01],\n        [ 4.7602e+01,  6.6926e+01,  3.0288e+01, -1.3895e+01, -1.3280e+02,\n         -2.1620e+00,  6.6151e+01, -2.5382e+01, -7.3098e-02,  1.1234e+02],\n        [ 3.5486e+01,  1.1517e+01, -3.5798e+01, -8.7197e+01, -6.4483e+01,\n          6.5144e+01,  4.1788e+01,  5.0956e+01, -5.2095e+00,  1.1533e+02],\n        [ 4.0934e+01,  3.8248e+01, -4.3809e+01, -4.0474e+00, -1.7485e+01,\n          4.1216e+01, -5.5216e+00, -5.1218e+01, -3.8729e+01, -1.4574e+01],\n        [ 5.7639e+01, -4.2511e+00, -7.2226e+00,  2.6966e+01, -2.1492e+01,\n         -3.5769e+01, -3.3954e+01,  2.9977e+01, -2.6904e+01, -3.0621e+01],\n        [-3.3285e+01, -1.4108e+01, -4.7646e+01,  3.7208e+01, -4.3689e+01,\n          6.4364e+01,  3.6912e+01, -7.0370e+01,  5.4739e+01, -2.7505e+01],\n        [-6.9992e+00,  1.9307e+01, -1.2342e+02, -6.6211e+01, -2.2006e+00,\n          3.0495e+00,  1.1288e+01,  2.0704e+00, -6.4240e+00, -5.3519e+01],\n        [-2.2302e+01, -2.2808e-01, -1.7332e+01, -1.7912e+01, -3.3511e+01,\n          4.4367e+01,  3.9043e+01, -1.2704e+02,  6.3262e+00, -2.0412e+01],\n        [ 2.9756e+01, -5.1026e+00, -8.0519e+01, -5.6198e+01,  1.7856e+01,\n          4.6548e+01,  1.9081e+01, -1.2328e+00, -5.4178e+01, -2.3121e+01],\n        [ 7.2224e+01,  1.7295e+01, -1.1332e+01, -1.8880e+01, -1.5918e+02,\n          3.7813e+01,  7.0238e+01,  2.3910e+01, -1.9812e+01,  4.2438e+01],\n        [ 7.4401e+01,  2.8340e+00, -1.0638e+02,  7.6696e+01, -5.8159e+01,\n          1.0469e+01, -2.0479e+01,  3.9304e+01,  8.3976e+00, -2.0666e+01],\n        [-1.1055e+01, -5.5809e+01, -3.6452e+01,  3.3901e+01, -4.3749e+01,\n          1.9431e+01, -2.2513e+00, -3.5193e+01,  2.9737e+01,  9.7065e+00],\n        [ 5.2389e+01, -3.6451e+01,  1.5358e+01,  7.8201e+00, -1.8485e+02,\n          3.7955e+00,  5.3735e+00, -3.7607e+01,  3.5625e+01, -3.9683e+01],\n        [ 7.9674e+00, -7.9373e+01, -2.2137e+01,  4.4688e+01, -9.7759e+01,\n          1.3530e+01, -4.6564e+00, -5.6338e+01, -4.8531e+01,  1.3495e+01],\n        [-4.6143e+01,  2.4228e+01, -3.8446e+01, -3.1261e+01, -1.4087e+01,\n          6.6833e+00, -7.8831e+00, -1.9526e+01, -1.6654e+01, -1.3568e+01],\n        [ 9.4608e+01, -1.5835e+01,  3.9010e+01, -3.0975e+00,  1.3518e+01,\n          1.0686e+01,  8.2890e+00,  6.3929e+01, -6.1648e+01, -2.5364e+01],\n        [ 6.8826e+01,  8.1642e+01, -3.5222e+01, -5.3220e+01, -5.8365e+01,\n          3.8822e+01, -4.2261e+01,  3.3996e+01, -2.1638e+01, -5.7187e+01],\n        [ 4.8576e-01, -3.4494e+01, -4.2121e+01,  1.9511e+01, -3.2839e+01,\n          7.4336e+01,  1.0796e+01, -1.1629e+00, -1.5517e+01, -9.2457e+01],\n        [ 6.7798e+00,  6.6608e+01, -7.4844e+01, -4.0441e+01, -3.5030e+01,\n          6.1988e+01, -3.0722e+00, -3.6164e+00,  1.8648e+01, -2.2319e+01],\n        [ 2.2849e+01, -3.1496e+01, -3.4339e+01, -1.9198e+01, -1.6128e+02,\n          3.7720e+01, -1.3757e+01, -1.0877e+02,  7.7823e+01, -3.5790e+00],\n        [ 7.1489e+01, -1.0742e+01, -3.6580e+01, -4.7453e+01, -2.6032e+01,\n          4.7259e+01,  8.0764e+00, -3.3934e+01, -1.1806e+01,  3.2263e+01],\n        [ 2.8705e+01, -5.7119e+01, -1.9080e+01,  2.8637e+01, -2.7450e+01,\n          6.8527e+00, -3.4822e+01, -1.7731e+01, -1.0232e+00, -1.2108e+01],\n        [ 7.1766e+01, -1.5878e+01, -6.1808e+01, -1.9247e+01, -1.0956e+02,\n          6.1981e+00, -1.8162e+01,  1.7948e+01,  4.2921e+00, -3.9544e+01],\n        [ 2.2705e+01,  4.0733e+01, -1.1469e+02,  3.9692e+01, -5.2911e+01,\n          3.0816e+01, -2.5026e+01,  1.7220e+01, -4.3147e+01,  9.8528e+00],\n        [ 2.8055e+01,  7.2936e+00, -1.7759e+01, -2.1806e+01,  2.0936e+01,\n         -4.1916e+01, -6.8795e+01, -3.6671e+00, -1.3471e+01,  3.8476e+00],\n        [ 4.7465e+01, -5.1519e+01, -9.5427e+01,  1.4658e+01, -7.9656e+01,\n          7.7583e+01,  3.5966e+01,  4.7771e+01, -6.9614e+01, -4.0254e+01],\n        [-3.6080e+01, -2.4745e+01, -6.5266e+01,  1.0064e+01, -3.7452e+01,\n          6.2068e+01,  2.8917e+00, -2.9924e+01,  3.8913e+01,  5.5596e+01],\n        [ 3.2243e+01, -4.4328e+01, -3.1293e+01,  7.4581e+00, -2.3907e+01,\n          4.3425e+01, -1.3941e+01,  2.2470e+01,  3.8165e+00, -4.1104e+01],\n        [ 3.3081e+01,  3.7735e+01, -4.0411e+01, -5.6604e+01, -4.7607e+01,\n         -1.4279e+01, -6.3391e+00, -5.0960e+01, -1.3042e+01,  1.4929e+01],\n        [ 3.1778e+01, -1.2900e+01,  1.1262e+00,  2.8500e+01, -1.7370e+01,\n          3.5686e+01,  9.7157e+00,  6.0141e+01, -2.2700e+00,  1.6741e+01],\n        [ 2.6651e+01,  8.7636e+00, -4.9065e+01,  3.9096e+01, -3.3829e+01,\n          2.9320e+01, -8.6123e-01, -6.5040e+00,  1.7807e+00, -3.5928e+01],\n        [ 3.1387e+01,  3.2301e+01, -6.6827e+01, -7.0443e+01,  4.3941e+00,\n          2.4638e+01, -1.5867e+01,  8.9600e+01, -3.1053e+01,  6.2410e+01],\n        [ 6.8537e+01,  1.4130e+01, -8.6545e+01, -2.4164e+01, -8.9639e+01,\n          3.5831e+01, -1.9748e+01,  2.6385e+01, -2.3977e+01,  1.3628e+01],\n        [ 3.6068e+01, -2.9635e+01, -1.3423e+01,  2.3151e+01, -4.5584e+01,\n          2.3153e+01, -4.0315e+01, -3.1305e+01,  4.1544e+00,  3.6242e+01],\n        [ 1.8989e+01,  6.8420e+01,  2.1520e+01,  2.8528e-01, -4.7706e+01,\n          3.3187e+01,  4.8366e+01, -1.9196e+01, -5.3203e+01, -1.7875e+01],\n        [ 9.0138e+01, -9.5584e-01, -1.8153e+01, -2.0085e+01, -1.3923e+01,\n          4.6147e+01, -6.2696e+00, -1.7873e+01, -4.0014e+01, -4.9256e+01],\n        [ 3.2717e+01,  1.9408e+01, -4.9903e+01, -9.0567e+01, -3.0216e+01,\n          4.3213e+01,  3.3022e+00,  1.6148e+01,  1.1160e+01,  8.1540e+01],\n        [ 6.3466e+01, -5.1439e+01, -6.7208e+01,  1.8386e+01, -5.9509e+01,\n          5.1944e+01,  2.6130e+00,  4.6383e+01, -2.7700e+01, -2.2843e+01],\n        [ 8.2082e+01, -3.5508e+01, -3.7524e+01, -4.3513e+00, -3.2677e+01,\n          4.6190e+01, -1.3662e+01, -7.4602e+00, -2.5886e+01, -8.5799e+01],\n        [ 2.9456e+01, -8.4827e+01, -5.9894e+01,  1.1526e+01, -1.0246e+02,\n          1.9255e+01, -2.1400e+01,  6.8360e+01,  3.5861e+00,  6.7396e+00],\n        [ 6.0168e+00,  3.3348e+01, -3.6264e+01, -7.1758e+01,  2.0009e+01,\n          6.0410e+01, -6.7034e+00,  5.6498e+00, -5.5303e+01,  2.0279e+01],\n        [ 3.9032e+01,  9.9936e+01, -8.7078e+00, -5.4328e+01,  4.6365e+00,\n          5.9877e+01,  4.2898e+00, -3.3044e+01, -1.9089e+01,  1.0612e+02],\n        [ 1.0821e+02,  1.8384e+01,  7.3369e-02,  5.6620e+01, -1.4214e+02,\n          3.6704e+01,  2.8180e+01, -5.1079e+01,  2.1180e+01,  1.2827e+01]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Apply Softmax to convert ouputs to probabilities:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F","execution_count":402,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply softmax for each output row\nprobs = F.softmax(outputs, dim=1)\n\nprint('batch 1:')\nprint('Outputs:\\n', probs.data)\nprint('Sum of one row:', sum(probs[0]).item())","execution_count":403,"outputs":[{"output_type":"stream","text":"batch 1:\nOutputs:\n tensor([[4.9721e-15, 7.8556e-01, 0.0000e+00, 1.4013e-45, 0.0000e+00, 5.2601e-19,\n         2.4350e-20, 6.0334e-19, 1.0140e-33, 2.1444e-01],\n        [6.4313e-10, 0.0000e+00, 5.9346e-10, 4.8520e-41, 0.0000e+00, 1.0000e+00,\n         2.7859e-23, 0.0000e+00, 2.6071e-19, 1.2871e-26],\n        [9.9811e-01, 3.2850e-37, 2.7725e-32, 1.8852e-03, 2.0801e-41, 3.1699e-16,\n         0.0000e+00, 3.1739e-27, 6.2636e-27, 2.2896e-29],\n        [1.0000e+00, 1.3129e-15, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         1.8070e-24, 7.0065e-45, 0.0000e+00, 4.5215e-24],\n        [2.8007e-01, 6.2027e-01, 1.8441e-16, 1.5702e-26, 8.6961e-07, 9.9656e-02,\n         1.7753e-18, 1.1951e-20, 1.8328e-18, 9.6348e-17],\n        [9.5037e-31, 4.4083e-15, 0.0000e+00, 1.1324e-17, 2.6072e-39, 1.0000e+00,\n         3.5026e-32, 1.8258e-34, 5.1948e-37, 0.0000e+00],\n        [6.5335e-26, 1.9606e-04, 2.6320e-25, 1.2066e-38, 0.0000e+00, 2.4511e-05,\n         4.6124e-04, 1.4292e-36, 2.6431e-39, 9.9932e-01],\n        [1.4013e-45, 1.5498e-30, 3.7669e-37, 0.0000e+00, 5.4212e-40, 9.1489e-28,\n         4.2498e-33, 0.0000e+00, 1.4799e-32, 1.0000e+00],\n        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 8.3739e-14, 0.0000e+00, 0.0000e+00],\n        [1.4118e-20, 0.0000e+00, 1.2209e-33, 1.0000e+00, 0.0000e+00, 9.3419e-31,\n         4.2237e-20, 4.5416e-18, 0.0000e+00, 1.7310e-40],\n        [1.5695e-42, 2.3262e-35, 1.5324e-28, 1.4263e-21, 0.0000e+00, 1.0000e+00,\n         1.5661e-24, 2.3042e-26, 1.4616e-42, 7.7382e-20],\n        [1.0138e-13, 2.0157e-16, 0.0000e+00, 3.3961e-15, 5.4114e-12, 8.1166e-13,\n         7.8864e-33, 1.7922e-37, 1.0000e+00, 1.1102e-18],\n        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8625e-43, 3.8445e-41,\n         0.0000e+00, 2.0127e-35, 2.6204e-43, 0.0000e+00],\n        [2.1099e-05, 1.9746e-36, 0.0000e+00, 0.0000e+00, 1.2491e-20, 3.5244e-10,\n         9.9998e-01, 6.2821e-26, 0.0000e+00, 2.9932e-42],\n        [3.3394e-11, 9.1138e-27, 0.0000e+00, 1.6895e-31, 0.0000e+00, 1.0000e+00,\n         0.0000e+00, 1.4013e-45, 2.6345e-32, 1.2718e-32],\n        [1.0000e+00, 3.0190e-31, 5.7560e-19, 1.6266e-11, 0.0000e+00, 2.6876e-25,\n         1.0557e-18, 0.0000e+00, 4.8610e-29, 5.6218e-32],\n        [1.0000e+00, 1.9521e-27, 0.0000e+00, 2.5644e-43, 0.0000e+00, 9.3004e-36,\n         1.1266e-23, 0.0000e+00, 5.5307e-37, 2.8026e-45],\n        [7.6708e-25, 3.0571e-17, 0.0000e+00, 0.0000e+00, 7.6221e-31, 1.0000e+00,\n         2.0763e-33, 3.6974e-19, 1.6518e-19, 1.0992e-35],\n        [2.1548e-19, 0.0000e+00, 1.8185e-21, 6.1090e-18, 0.0000e+00, 1.0000e+00,\n         8.1666e-29, 0.0000e+00, 6.0348e-31, 4.9291e-18],\n        [1.0000e+00, 2.7606e-43, 2.1527e-33, 4.2441e-37, 1.4013e-45, 1.2303e-13,\n         3.9620e-20, 1.0661e-24, 5.6052e-45, 0.0000e+00],\n        [9.9932e-01, 0.0000e+00, 0.0000e+00, 2.5033e-10, 2.2701e-43, 6.7607e-04,\n         5.4605e-26, 5.2346e-41, 1.0930e-43, 0.0000e+00],\n        [1.0000e+00, 3.0925e-14, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9298e-38,\n         9.6413e-24, 3.6431e-20, 0.0000e+00, 7.0922e-29],\n        [1.0000e+00, 0.0000e+00, 1.5026e-28, 1.4868e-42, 7.0065e-45, 8.3587e-30,\n         3.8937e-40, 2.3884e-27, 0.0000e+00, 0.0000e+00],\n        [9.1982e-17, 5.4735e-15, 9.4785e-18, 1.1854e-37, 4.3665e-22, 9.9999e-01,\n         4.6116e-18, 4.8825e-21, 2.5654e-13, 1.4845e-05],\n        [1.0894e-11, 9.2064e-05, 0.0000e+00, 0.0000e+00, 3.8424e-29, 8.2951e-03,\n         4.0152e-12, 9.8737e-01, 4.2403e-03, 2.0641e-13],\n        [1.0000e+00, 3.9898e-16, 8.5012e-13, 1.4861e-29, 4.2927e-21, 5.2101e-40,\n         0.0000e+00, 5.3803e-10, 8.8433e-38, 1.0432e-29],\n        [7.7778e-04, 9.9922e-01, 9.4888e-23, 1.2937e-35, 3.3042e-06, 5.9280e-15,\n         4.5187e-30, 4.8563e-20, 3.6246e-36, 7.5670e-44],\n        [1.0000e+00, 4.5603e-30, 2.2465e-30, 2.0491e-16, 1.2163e-42, 4.3624e-08,\n         0.0000e+00, 4.4628e-31, 4.2039e-45, 0.0000e+00],\n        [1.6921e-08, 9.1727e-17, 4.1589e-41, 1.5924e-04, 7.4412e-19, 9.9984e-01,\n         3.2092e-10, 5.9363e-23, 5.2967e-10, 7.7066e-36],\n        [1.7784e-20, 1.6845e-41, 8.3237e-43, 3.9796e-11, 8.0334e-29, 1.0037e-20,\n         3.0506e-42, 1.0000e+00, 1.4013e-45, 3.8359e-36],\n        [7.0908e-09, 0.0000e+00, 0.0000e+00, 9.3938e-26, 0.0000e+00, 1.0000e+00,\n         0.0000e+00, 6.7650e-38, 3.4728e-16, 4.5828e-39],\n        [3.5776e-30, 1.2337e-15, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n         3.6928e-28, 2.9743e-11, 0.0000e+00, 4.9802e-42],\n        [1.0000e+00, 3.7531e-10, 0.0000e+00, 5.2532e-41, 0.0000e+00, 1.1208e-14,\n         0.0000e+00, 2.9280e-14, 1.2312e-20, 0.0000e+00],\n        [6.6728e-18, 1.0980e-33, 3.8315e-39, 2.3383e-07, 0.0000e+00, 2.2459e-39,\n         3.3630e-39, 1.0000e+00, 4.4166e-36, 1.1210e-44],\n        [9.2128e-10, 5.9283e-11, 2.3660e-25, 1.3879e-18, 0.0000e+00, 1.0000e+00,\n         2.7325e-43, 9.9136e-11, 1.6036e-21, 0.0000e+00],\n        [2.5401e-23, 1.1799e-36, 4.3127e-34, 5.9664e-27, 3.3858e-37, 1.0000e+00,\n         8.0066e-27, 1.2736e-09, 3.0174e-40, 0.0000e+00],\n        [6.3701e-40, 0.0000e+00, 1.1535e-38, 3.1579e-27, 0.0000e+00, 1.0195e-21,\n         7.5191e-17, 0.0000e+00, 1.0000e+00, 2.6290e-17],\n        [1.0000e+00, 0.0000e+00, 1.1658e-23, 1.3530e-21, 0.0000e+00, 1.7473e-37,\n         1.6990e-09, 0.0000e+00, 5.0690e-11, 4.1638e-37],\n        [1.0386e-18, 8.4228e-35, 0.0000e+00, 5.7033e-43, 0.0000e+00, 1.0000e+00,\n         0.0000e+00, 0.0000e+00, 5.9382e-37, 0.0000e+00],\n        [2.5389e-33, 1.0000e+00, 1.1549e-39, 2.5564e-34, 1.0426e-10, 3.8081e-22,\n         3.3357e-25, 3.7858e-24, 7.1544e-27, 1.0320e-23],\n        [7.1050e-21, 7.7996e-42, 0.0000e+00, 0.0000e+00, 7.3784e-33, 1.9877e-04,\n         3.4947e-16, 5.6716e-11, 3.0933e-32, 9.9980e-01],\n        [1.0000e+00, 8.2677e-30, 0.0000e+00, 9.2716e-36, 5.8456e-29, 6.0150e-22,\n         2.4147e-28, 1.2108e-31, 1.4259e-17, 1.6438e-31],\n        [1.0000e+00, 0.0000e+00, 0.0000e+00, 8.1293e-31, 0.0000e+00, 1.2717e-12,\n         0.0000e+00, 1.3418e-30, 1.4013e-45, 0.0000e+00],\n        [1.0000e+00, 1.5883e-29, 1.5164e-29, 4.4029e-25, 1.1507e-36, 4.1382e-09,\n         1.0076e-27, 4.2767e-34, 9.4805e-31, 5.3355e-13],\n        [4.7960e-21, 1.0000e+00, 2.4174e-35, 0.0000e+00, 2.1936e-35, 3.9480e-17,\n         5.1870e-23, 0.0000e+00, 1.3885e-36, 4.5224e-38],\n        [4.1573e-13, 5.1336e-15, 4.5960e-12, 1.9219e-17, 0.0000e+00, 2.4204e-06,\n         6.4234e-15, 6.5067e-29, 9.9994e-01, 5.4409e-05],\n        [9.4490e-12, 3.6844e-23, 0.0000e+00, 9.3319e-34, 0.0000e+00, 1.0000e+00,\n         1.7144e-26, 4.4943e-34, 3.7265e-36, 9.7048e-26],\n        [1.0000e+00, 0.0000e+00, 1.0402e-27, 5.1153e-17, 0.0000e+00, 3.4686e-12,\n         1.9505e-33, 5.6052e-45, 6.3300e-08, 2.8659e-38],\n        [9.9986e-01, 1.1878e-10, 9.1530e-38, 1.9869e-38, 0.0000e+00, 1.8572e-21,\n         4.9779e-19, 3.7275e-43, 1.4272e-04, 1.6763e-19],\n        [1.0000e+00, 1.0589e-31, 0.0000e+00, 2.3882e-23, 0.0000e+00, 2.0056e-18,\n         4.2908e-42, 1.0388e-41, 0.0000e+00, 1.4013e-45],\n        [9.9738e-01, 8.9999e-14, 2.0863e-24, 3.9206e-23, 9.9681e-26, 2.6182e-03,\n         1.9333e-18, 1.0394e-20, 3.2223e-33, 0.0000e+00],\n        [6.4985e-08, 9.3105e-01, 0.0000e+00, 7.7235e-41, 1.7105e-39, 4.4913e-02,\n         4.6095e-26, 2.4035e-02, 1.8151e-27, 1.0370e-43],\n        [5.0545e-23, 1.9291e-31, 9.3229e-40, 0.0000e+00, 0.0000e+00, 5.6574e-13,\n         3.0446e-22, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n        [9.9999e-01, 3.0513e-37, 1.6664e-14, 3.4980e-15, 0.0000e+00, 1.3742e-05,\n         5.9650e-33, 6.2204e-33, 6.3747e-14, 1.1939e-16],\n        [1.2079e-06, 2.1788e-25, 7.1509e-24, 0.0000e+00, 5.5538e-07, 1.0000e+00,\n         1.1622e-17, 1.4900e-09, 2.1437e-39, 3.4727e-18],\n        [4.0744e-24, 5.4265e-26, 2.9427e-44, 9.1774e-22, 7.6264e-35, 1.0470e-18,\n         7.4115e-24, 4.4050e-17, 0.0000e+00, 1.0000e+00],\n        [3.2026e-20, 9.1433e-09, 5.2120e-29, 5.7146e-22, 5.5083e-33, 5.6095e-20,\n         2.0521e-01, 5.1583e-28, 7.9479e-01, 6.2148e-15],\n        [1.5013e-06, 4.1989e-25, 0.0000e+00, 0.0000e+00, 1.4259e-34, 1.7729e-38,\n         3.8790e-35, 5.1499e-17, 0.0000e+00, 1.0000e+00],\n        [7.6492e-29, 1.8885e-20, 2.3144e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         8.6971e-21, 0.0000e+00, 0.0000e+00, 1.0000e+00],\n        [2.1154e-35, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6051e-22,\n         1.1543e-32, 1.1060e-28, 0.0000e+00, 1.0000e+00],\n        [4.1774e-01, 2.8448e-02, 6.5643e-38, 1.2178e-20, 1.7768e-26, 5.5381e-01,\n         2.7884e-21, 3.9780e-41, 1.0559e-35, 3.2650e-25],\n        [1.0000e+00, 1.3222e-27, 6.7729e-29, 4.7735e-14, 4.3022e-35, 2.7117e-41,\n         1.6655e-40, 9.6898e-13, 1.9193e-37, 4.6664e-39],\n        [3.9096e-43, 8.3204e-35, 0.0000e+00, 1.6086e-12, 0.0000e+00, 9.9993e-01,\n         1.1968e-12, 0.0000e+00, 6.6057e-05, 1.2648e-40],\n        [3.7614e-12, 9.9967e-01, 0.0000e+00, 7.2438e-38, 4.5639e-10, 8.6983e-08,\n         3.2928e-04, 3.2675e-08, 6.6855e-12, 2.3555e-32],\n        [1.1065e-29, 4.2717e-20, 1.5934e-27, 8.9256e-28, 1.4998e-34, 9.9515e-01,\n         4.8522e-03, 0.0000e+00, 2.9999e-17, 7.3274e-29],\n        [5.0975e-08, 3.7009e-23, 0.0000e+00, 2.8026e-45, 3.4589e-13, 1.0000e+00,\n         1.1784e-12, 1.7739e-21, 1.8217e-44, 5.5318e-31],\n        [8.7935e-01, 1.2265e-24, 4.5310e-37, 2.3901e-40, 0.0000e+00, 9.9939e-16,\n         1.2065e-01, 9.1588e-22, 9.4087e-41, 1.0190e-13],\n        [9.1577e-02, 7.5947e-33, 0.0000e+00, 9.0842e-01, 0.0000e+00, 1.5709e-29,\n         5.7033e-43, 5.2402e-17, 1.9804e-30, 4.7224e-43],\n        [2.9463e-20, 1.0789e-39, 2.7520e-31, 9.8469e-01, 1.8640e-34, 5.1206e-07,\n         1.9620e-16, 9.6922e-31, 1.5313e-02, 3.0614e-11],\n        [1.0000e+00, 2.6150e-39, 8.2773e-17, 4.4063e-20, 0.0000e+00, 7.8751e-22,\n         3.8155e-21, 8.2322e-40, 5.2413e-08, 1.0322e-40],\n        [1.1280e-16, 0.0000e+00, 9.5047e-30, 1.0000e+00, 0.0000e+00, 2.9377e-14,\n         3.7144e-22, 1.4013e-44, 3.2751e-41, 2.8367e-14],\n        [2.7453e-31, 1.0000e+00, 6.0448e-28, 7.9693e-25, 2.2914e-17, 2.4022e-08,\n         1.1338e-14, 9.9542e-20, 1.7591e-18, 3.8522e-17],\n        [1.0000e+00, 0.0000e+00, 7.1444e-25, 3.6854e-43, 6.0674e-36, 3.5715e-37,\n         3.2505e-38, 4.7428e-14, 0.0000e+00, 0.0000e+00],\n        [2.7171e-06, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5333e-19,\n         0.0000e+00, 2.0301e-21, 1.4013e-45, 0.0000e+00],\n        [8.4617e-33, 0.0000e+00, 0.0000e+00, 1.5482e-24, 0.0000e+00, 1.0000e+00,\n         2.5425e-28, 1.6272e-33, 9.4978e-40, 0.0000e+00],\n        [1.0293e-26, 9.9025e-01, 0.0000e+00, 0.0000e+00, 7.0065e-45, 9.7503e-03,\n         5.4187e-31, 3.1444e-31, 1.4689e-21, 2.3717e-39],\n        [1.3342e-24, 0.0000e+00, 0.0000e+00, 7.3148e-43, 0.0000e+00, 3.8345e-18,\n         1.6884e-40, 0.0000e+00, 1.0000e+00, 4.4437e-36],\n        [1.0000e+00, 1.9391e-36, 0.0000e+00, 0.0000e+00, 4.4421e-43, 2.9987e-11,\n         2.8854e-28, 0.0000e+00, 6.6873e-37, 9.2127e-18],\n        [5.1703e-01, 2.7585e-38, 9.1360e-22, 4.8297e-01, 2.1172e-25, 1.6714e-10,\n         1.3301e-28, 3.5199e-21, 6.3478e-14, 9.7370e-19],\n        [1.0000e+00, 8.6422e-39, 0.0000e+00, 2.9735e-40, 0.0000e+00, 3.3428e-29,\n         8.8008e-40, 4.2377e-24, 4.9701e-30, 0.0000e+00],\n        [1.0947e-08, 7.3902e-01, 0.0000e+00, 2.6094e-01, 1.5842e-41, 3.6481e-05,\n         2.0423e-29, 4.5414e-11, 2.7562e-37, 2.8688e-14],\n        [9.9919e-01, 9.6209e-10, 1.2675e-20, 2.2146e-22, 8.0901e-04, 4.0894e-31,\n         8.6740e-43, 1.6712e-14, 9.2342e-19, 3.0662e-11],\n        [8.3137e-14, 0.0000e+00, 0.0000e+00, 4.6960e-28, 0.0000e+00, 1.0000e+00,\n         8.4269e-19, 1.1284e-13, 0.0000e+00, 0.0000e+00],\n        [2.3682e-43, 1.9822e-38, 0.0000e+00, 2.5980e-23, 6.0256e-44, 9.9846e-01,\n         1.9933e-26, 1.1170e-40, 8.7744e-11, 1.5438e-03],\n        [1.3931e-05, 7.7520e-39, 3.5527e-33, 2.3985e-16, 5.7291e-30, 9.9999e-01,\n         1.2199e-25, 7.9381e-10, 6.2866e-18, 1.9477e-37],\n        [9.4408e-03, 9.9056e-01, 1.1420e-34, 1.0592e-41, 8.5594e-38, 2.5516e-23,\n         7.1591e-20, 2.9941e-39, 8.7857e-23, 1.2345e-10],\n        [4.8080e-13, 1.8994e-32, 2.3446e-26, 1.8121e-14, 2.1740e-34, 2.3940e-11,\n         1.2602e-22, 1.0000e+00, 7.8540e-28, 1.4173e-19],\n        [3.9382e-06, 6.7116e-14, 5.1550e-39, 9.9994e-01, 2.1339e-32, 5.6805e-05,\n         4.4341e-18, 1.5710e-20, 6.2257e-17, 2.6159e-33],\n        [5.2336e-26, 1.3050e-25, 0.0000e+00, 0.0000e+00, 9.9026e-38, 6.1321e-29,\n         0.0000e+00, 1.0000e+00, 0.0000e+00, 1.5551e-12],\n        [1.0000e+00, 2.3497e-24, 0.0000e+00, 5.4966e-41, 0.0000e+00, 6.2461e-15,\n         4.5509e-39, 4.9371e-19, 6.6293e-41, 1.4226e-24],\n        [4.5663e-01, 1.3337e-29, 1.4647e-22, 1.1222e-06, 1.5803e-36, 1.1239e-06,\n         3.0671e-34, 2.5109e-30, 6.3060e-15, 5.4337e-01],\n        [3.4060e-22, 1.0000e+00, 4.2790e-21, 2.5660e-30, 0.0000e+00, 4.9898e-16,\n         1.9513e-09, 8.8820e-39, 0.0000e+00, 3.3304e-38],\n        [1.0000e+00, 2.7432e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8478e-20,\n         1.3509e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [6.2567e-22, 1.0383e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2634e-17,\n         1.0509e-34, 3.9850e-29, 2.7174e-31, 1.0000e+00],\n        [9.9999e-01, 0.0000e+00, 0.0000e+00, 2.6445e-20, 0.0000e+00, 9.9117e-06,\n         3.7325e-27, 3.8102e-08, 2.5550e-40, 3.2854e-38],\n        [1.0000e+00, 0.0000e+00, 0.0000e+00, 2.8993e-38, 0.0000e+00, 2.5828e-16,\n         2.6232e-42, 1.2945e-39, 0.0000e+00, 0.0000e+00],\n        [1.2715e-17, 0.0000e+00, 0.0000e+00, 2.0772e-25, 0.0000e+00, 4.7218e-22,\n         1.0416e-39, 1.0000e+00, 7.4003e-29, 1.7330e-27],\n        [2.3853e-24, 1.7669e-12, 1.0356e-42, 0.0000e+00, 2.8449e-18, 1.0000e+00,\n         7.1320e-30, 1.6524e-24, 0.0000e+00, 3.7298e-18],\n        [7.2677e-30, 2.0497e-03, 0.0000e+00, 0.0000e+00, 8.4078e-45, 8.2073e-21,\n         5.6052e-45, 0.0000e+00, 0.0000e+00, 9.9795e-01],\n        [1.0000e+00, 9.7399e-40, 0.0000e+00, 3.9302e-23, 0.0000e+00, 8.8081e-32,\n         1.7495e-35, 0.0000e+00, 1.5948e-38, 3.7611e-42]])\nSum of one row: 1.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_probs, preds = torch.max(probs, dim=1)\nprint(preds)\nprint(max_probs)","execution_count":404,"outputs":[{"output_type":"stream","text":"tensor([1, 5, 0, 0, 1, 5, 9, 9, 0, 3, 5, 8, 0, 6, 5, 0, 0, 5, 5, 0, 0, 0, 0, 5,\n        7, 0, 1, 0, 5, 7, 5, 5, 0, 7, 5, 5, 8, 0, 5, 1, 9, 0, 0, 0, 1, 8, 5, 0,\n        0, 0, 0, 1, 7, 0, 5, 9, 8, 9, 9, 9, 5, 0, 5, 1, 5, 5, 0, 3, 3, 0, 3, 1,\n        0, 1, 5, 1, 8, 0, 0, 0, 1, 0, 5, 5, 5, 1, 7, 3, 7, 0, 9, 1, 0, 9, 0, 0,\n        7, 5, 9, 0])\ntensor([0.7856, 1.0000, 0.9981, 1.0000, 0.6203, 1.0000, 0.9993, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 0.9993, 1.0000, 1.0000, 1.0000, 0.9874, 1.0000, 0.9992,\n        1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000,\n        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9974, 0.9311, 1.0000, 1.0000,\n        1.0000, 1.0000, 0.7948, 1.0000, 1.0000, 1.0000, 0.5538, 1.0000, 0.9999,\n        0.9997, 0.9951, 1.0000, 0.8793, 0.9084, 0.9847, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 0.9902, 1.0000, 1.0000, 0.5170, 1.0000, 0.7390,\n        0.9992, 1.0000, 0.9985, 1.0000, 0.9906, 1.0000, 0.9999, 1.0000, 1.0000,\n        0.5434, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9980,\n        1.0000], grad_fn=<MaxBackward0>)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Loss Function: Cross Entropy"},{"metadata":{},"cell_type":"markdown","source":"$$ D(\\hat{\\mathbf y}, \\mathbf y) = - \\sum_j y_j \\ln[\\hat y] $$\n\nExample:\n\n$\n\\mathbf y =\n\\begin{bmatrix}\n0\\\\ \n1\\\\ \n0\\\\ \n0\n\\end{bmatrix}\n$\n\n$\n\\hat{\\mathbf y} =\n\\begin{bmatrix}\n0.18\\\\ \n0.41\\\\ \n0.16\\\\ \n0.25\n\\end{bmatrix}\n$\n\n$\nD(\\hat{\\mathbf y}, \\mathbf y) = -(0*\\ln[0.18] + 1*\\ln[0.41] + 0*\\ln[0.16] + 0*\\ln[0.25]) = - \\ln[0.41]\n$"},{"metadata":{},"cell_type":"markdown","source":"PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the `torch.nn.functional` package. Moreover, it also performs softmax internally, so we can directly pass in the model's outputs without converting them into probabilities."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn = F.cross_entropy","execution_count":405,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss for current batch of data\nloss = loss_fn(outputs, labels)\nloss","execution_count":406,"outputs":[{"output_type":"execute_result","execution_count":406,"data":{"text/plain":"tensor(66.9670, grad_fn=<NllLossBackward>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return float(torch.sum(preds == labels) / float(len(preds)))","execution_count":407,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test accuracy\naccuracy(\n        torch.tensor([[0.0000e+00, 3.2972e-27, 8.5308e-22, 1.0000e+00, 0.0000e+00, 4.1089e-36,\n                     2.5768e-15, 1.0142e-23, 1.7163e-16, 0.0000e+00],\n                    [7.8614e-34, 4.0738e-30, 5.5259e-13, 4.1027e-17, 8.0373e-31, 1.1074e-26,\n                     9.0269e-01, 1.5159e-09, 9.7312e-02, 7.3484e-36],\n                    [7.2744e-38, 1.9258e-20, 1.0000e+00, 1.5459e-20, 6.1245e-37, 3.6680e-15,\n                     1.2950e-27, 2.5952e-12, 3.5957e-27, 2.1079e-39]]),\n        torch.tensor([3., 6, 4])\n                    )# 3,6,2 are the correct labels","execution_count":408,"outputs":[{"output_type":"execute_result","execution_count":408,"data":{"text/plain":"0.6666666865348816"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = torch.optim.SGD(model.parameters(), lr=1e-5)","execution_count":409,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(epochs, model, train_dl, optimizer):\n    history = []\n    \n    # Initial Validation\n    for xv, yv in val_dl:\n        preds_val = model(xv)\n        acc = accuracy(preds_val, yv)\n        history.append(acc)\n        break\n    print('Epoch 0: {}'.format(acc))\n    \n    for epoch in range(epochs):\n        \n        # Training\n        for xb, yb in train_dl:\n            preds = model(xb)\n            loss = loss_fn(preds, yb)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        # Validation\n        for xv, yv in val_dl:\n            preds_val = model(xv)\n            acc = accuracy(preds_val, yv)\n            history.append(acc)\n            break\n            \n        if (epoch+1) % 5 == 0 or (epoch+1) in range(1, 5):\n            print('Epoch {}: {}'.format(epoch+1, acc))\n            \n    return history","execution_count":410,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 25\nhistory = fit(epochs, model, train_dl, opt)","execution_count":411,"outputs":[{"output_type":"stream","text":"Epoch 0: 0.0794999971985817\nEpoch 1: 0.7105000019073486\nEpoch 2: 0.7817500233650208\nEpoch 3: 0.8075000047683716\nEpoch 4: 0.8242499828338623\nEpoch 5: 0.8352500200271606\nEpoch 10: 0.8522499799728394\nEpoch 15: 0.8627499938011169\nEpoch 20: 0.8667500019073486\nEpoch 25: 0.8677499890327454\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.arange(1, epochs+2), history)\nplt.gca().set_ylim(top=1, bottom=0)\n\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.show()","execution_count":412,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAcrklEQVR4nO3de5hcdZ3n8fe3bt3p7ly7mwzkIoEJoIggZIKOyqiMcnGYrI+LXHSdQWdiHPCyuzrw7D67zo57cdRZZ1EkREVlZEFd1GEVFUVGdBUnCSIkRCRGJDdNV+fW1Z1UdVV9949zulPdqe5UQk5Xd/0+r+fpp845dbr6e1JPzuec3zm/3zF3R0REwpVqdgEiItJcCgIRkcApCEREAqcgEBEJnIJARCRwCgIRkcAlFgRmdqeZ7TGzTRO8b2Z2q5ltNbMnzOzCpGoREZGJJXlG8Hng8knevwJYHv+sBm5PsBYREZlAYkHg7o8AeydZZRVwl0ceBeaZ2alJ1SMiIvVlmvi3FwHba+Z3xMt2j1/RzFYTnTXQ2dl50TnnnDMlBYqItIqNGzfm3b233nvNDAKrs6zueBfuvg5YB7BixQrfsGFDknWJiLQcM/vNRO81866hHcCSmvnFwK4m1SIiEqxmBsH9wNviu4deBhxw96OahUREJFmJNQ2Z2T3Aq4EeM9sBfBDIArj7WuAB4EpgKzAE3JBULSIiMrHEgsDdrzvG+w7cmNTfFxGRxqhnsYhI4BQEIiKBUxCIiAROQSAiEjgFgYhI4BQEIiKBUxCIiAROQSAiEjgFgYhI4BQEIiKBUxCIiAROQSAiEjgFgYhI4BQEIiKBUxCIiAROQSAiEjgFgYhI4BQEIiKBUxCIiAROQSAiEjgFgYhI4BQEIiKByzS7ABEJj7tTrjrDlSrDZWe4WqVUjn8q0WuxXGW4cvTykdfhysiPj5kulauUq/HnVqL1yxUnkzZ6utrond1GT1eO7s42euLpnq422rPpSWuuVp39h4bpLxTpKxTJF0rkB4rkC0X6CyX2DZWoVJ2qOxWPtrHqHi+L5munqw4pg3TKSKeMlBmZdPw6wbJLX7iQq84/7aR/HwoCkRnC3RkoluOdT4n+QrQT6iuUKA5XyGVS5NKp6HXkJ506enk6RSZtDFeiHdNwvKMsV51ytRovc8qVarQsfi2WqxSHKxQrVYrD0Y66WK7Ey2um4/XG7qRrd9bR/MmWSRnZdIpseuQ1RTZjZFPRdKlSJT9QZKBYrvv7s9sydMeh0NPVxqxcmv7BIzv7vYMlytWj606njO7OHAs6c6M7bTMjbZCyaD6VglQqRVvGsHjnb0DVoepOueJU3CkOR//WI8tGgqRSjd4/59Q5J/3fDRQEIsdUqXq0k4t3fqVy9bh+34mPTMt+5Ki2XKVUqcSvfmRZuUKpUuXAoWHyAyXy8c4+XyjRVyjW/dtmkIt3dH7y969Hacukop9smvZsirZM+siyTJr5HVly6TZymdTRO+Z4OpNOkauZzqbtqLAamW7LpMil02QzY9dpS6fJpI/s/M2sofoPD1dGj+LH/PsOFEd3/L/qKzBUqtDTlePUue2ct2jukZCIzyJ6u9ro7mpj3qwsqVRjf3u6UhDIjFeuVDl4uMz+oRL7Dw1z8NAwh4crHBquMFSqcKhU4fDI9HDNdM38REe1xXK17lFg0jIpo7um+eLMU7rojY9Ue2YfOWrt7sqxoCNHJp0abW6ZqCmldrpcdbIpI5NOkU4Z2bSRSUVnCpnUuOn4DKIt3kE3usOdrtqzaRbP72Dx/I5mlzJtKAhkypTKVfYNlThUGt9scKQd98h81CQxXKlSKFY4EO/k9w8Ns2+oxIF4ev9QiYOH65/qj5dOGR3ZNLNy8U82TXs2eu3qzERHttloZ9c25kg3Hc9H09n49P94ZDPREXAuProdf+TbFs9n4/mObPq4jzLNbPSIu7PtuH5VAqcgkOM2XKlGR9KlCoOlCvuHSuwdLNE/WGLf4Njp/sHoItreQmnCttlGmMHcWVnmzcoytyNqjz2jp5N5HTnmdUTL53XkmNuRZU57lo54R1+708+mdZOcSD0KggAdKlXGtI3mC8UjF8SGhhkqljkUN62MNJ/Uvh6rqSSXSdHdmWN+R47urhxLF3SwIL6YNr8zR2cufVSbcXQkHDVJjEyPtB93ZNPMmZUlPcPbYUWmKwVBCxosltmy+yCbdh5ga1+BvoGxO/zBUqXu781pz9Dd1UZHLk1HLk1XW4beeH5W7kgzSsfIdHykPT8+Qh/56cilZ3w7skhIFAQz3MHDw2zeeZDNuw6waecBNu06yK/6CqN3j8zryHLK7OjC4vmL54272Dj2omNbZvL7qEWkNSkIZpBCsczjz+1n064DPLnzAJt3HuDZ/qHR90+d2865p83lqpecxosXzeHFi+aycE57EysWkZlAQTDNbd87xPe2/I7v/2IPj27rH+2Is3j+LM5bNJerVyzh3NOinX5Pl24VEZHjpyCYZipV52fP7eN7W/bw0Jbf8cyeAgBn9nZywyuW8arlPZy3aC7zOnJNrlREWoWCYBo4eHiYR37Zx/e37OHhp/ewb2iYTMpYuWwB1/zBEv74hQs5vaez2WWKSItKNAjM7HLgfwFp4DPu/uFx788FvggsjWv5mLt/LsmapovDwxW++thOvvnkLn66bS/lqjOvI8trzj6FS194Cq9a3svcWdlmlykiAUgsCMwsDdwGvA7YAaw3s/vd/ama1W4EnnL3q8ysF3jazO5291JSdTXbvsES//job/jCj5+lf7DEmb2dvONVy/jjFy7kwqXzda+8iEy5JM8IVgJb3X0bgJndC6wCaoPAgdkW3XTeBewFTrz76TS2fe8Qn/3Rr/nS+u0cGq7w2nNOYfUlZ3DxsgW6515EmirJIFgEbK+Z3wFcPG6dTwL3A7uA2cA17n7U8IpmthpYDbB06dJEik3Kpp0HWPfINr755G4MWHXBIlZfcgZn/97sZpcmIgIkGwT1DnPHj01wGfA48FrgTOC7ZvZDdz845pfc1wHrAFasWDH1Q0EeJ3fnR1vz3PGDbfxoa56utgzveOUybnjF6Zw6d1azyxMRGSPJINgBLKmZX0x05F/rBuDD7u7AVjP7NXAO8C8J1pWYcqXKN5/czR0/2MZTuw9yyuw2br78HK6/eKku/IrItJVkEKwHlpvZMmAncC1w/bh1ngMuBX5oZguBs4FtCdaUmH2DJd58x094Zk+BM3s7+cibXsKql56mYRtEZNpLLAjcvWxmNwHfIbp99E5332xma+L31wIfAj5vZk8SNSXd7O75pGpKSqXqvOfen/Gb/iE+9ZYLufzc35vxTywSkXAk2o/A3R8AHhi3bG3N9C7g9UnWMBU+/t1f8sNn8vz3N57Hleed2uxyRESOi57U8Tw9uPm3fPLhrbx5xWKuW7nk2L8gIjLNKAieh219Bf79l3/OeYvm8rerXqz+ACIyIykITtBgscyaL24kkzZuf+uFtGd1UVhEZiYNOncC3J2/vu8Jtu4pcNfbL2bx/I5mlyQicsJ0RnACPvujX/PNJ3bz/svO5pXLe5pdjojI86IgOE4/+VU//+Nbv+Cycxfyrj86s9nliIg8bwqC47D7wCHefc9jvKC7g49dfb4uDotIS1AQNKhYrvBXdz/GUKnCHW+9iNntGjJCRFqDLhY36EPfeIqfPbefT73lQpYv1MihItI6dEbQgK9s2M4XH32Od15yhnoOi0jLURAcw6adB/iPX9/Ey8/o5gOXnd3sckRETjoFwST2DZZ45z9upLszxyeufymZtP65RKT16BrBBKpV571fepy+gSJfXvNyerraml2SiEgidIg7gWf2FHjkl328/7KzuGDJvGaXIyKSGAXBBPYMHAbggiXzm1yJiEiyFAQTyBeKAHR35ZpciYhIshQEE8gPlAB0bUBEWp6CYAL5QpFcOsWcdl1PF5HWpiCYQL5Qoqcrp/GERKTlKQgmkC8U6VazkIgEQEEwgXyhSI8uFItIABQEE+gvlHShWESCoCCow93pH1TTkIiEQUFQx4FDwwxXXE1DIhIEBUEdI53JemfrjEBEWp+CoI58QZ3JRCQcCoI6NLyEiIREQVBHfiAKAp0RiEgIFAR19A+WSBnM79AZgYi0PgVBHflCkQWdOdIpDS8hIq1PQVBH34A6k4lIOBQEdUTDSygIRCQMCoI6+gc1zpCIhENBUEd+oKThJUQkGIkGgZldbmZPm9lWM7tlgnVebWaPm9lmM/tBkvU0YrBY5tBwRU1DIhKMxB6/ZWZp4DbgdcAOYL2Z3e/uT9WsMw/4FHC5uz9nZqckVU+j+kd7FatpSETCkOQZwUpgq7tvc/cScC+watw61wNfdffnANx9T4L1NKSvoM5kIhKWJINgEbC9Zn5HvKzWWcB8M/tnM9toZm+r90FmttrMNpjZhr6+voTKjeQVBCISmCSDoF5vLB83nwEuAt4AXAb8JzM766hfcl/n7ivcfUVvb+/Jr7TGaNPQbDUNiUgYjhkEZvYnZnYigbEDWFIzvxjYVWedb7v7oLvngUeA80/gb500owPOdeqMQETC0MgO/lrgGTP7iJm98Dg+ez2w3MyWmVku/pz7x63zT8CrzCxjZh3AxcCW4/gbJ12+UGROe4ZcRnfWikgYjnnXkLu/1czmANcBnzMzBz4H3OPuA5P8XtnMbgK+A6SBO919s5mtid9f6+5bzOzbwBNAFfiMu296/pt14vKFIj16II2IBKSh20fd/aCZ3QfMAt4HvBH4gJnd6u6fmOT3HgAeGLds7bj5jwIfPd7Ck5LXQ+tFJDCNXCO4ysy+BnwfyAIr3f0Korb89ydc35SLxhnShWIRCUcjZwRXAx9390dqF7r7kJm9PZmymic/UKTn93uaXYaIyJRpJAg+COwemTGzWcBCd3/W3R9KrLImKJWrHDxcVtOQiASlkVtjvkJ0IXdEJV7WcvoH1ZlMRMLTSBBk4iEiAIinW7IRPT8QbaYeWi8iIWkkCPrM7E9HZsxsFZBPrqTm0fASIhKiRq4RrAHuNrNPEg0bsR2oOybQTDcSBL0KAhEJSCMdyn4FvMzMugCbrBPZTJcvqGlIRMLTUIcyM3sDcC7QbhaNJefuf5tgXU2RLxSZlU3T2ZbYYxpERKadRjqUrQWuAd5N1DR0NfCChOtqiv5CUaOOikhwGrlY/Ifu/jZgn7v/F+DljB1VtGVoeAkRCVEjQXA4fh0ys9OAYWBZciU1T75Q1PDTIhKcRoLg/8bPFv4o8BjwLHBPkkU1S75QpFdNQyISmEmvisYPpHnI3fcD95nZN4B2dz8wJdVNoUrV2TuopiERCc+kZwTuXgX+vma+2IohALBvqETV1ZlMRMLTSNPQg2b2Jhu5b7RFjT6iUn0IRCQwjdww/++ATqBsZoeJbiF1d5+TaGVTbPSh9TojEJHANNKzePZUFNJsGmdIREJ1zCAws0vqLR//oJqZrm9gJAjUNCQiYWmkaegDNdPtwEpgI/DaRCpqknyhRDZtzJ2VbXYpIiJTqpGmoatq581sCfCRxCpqkv64M1mLXxMXETlKI3cNjbcDePHJLqTZ8hpnSEQC1cg1gk8AHs+mgAuAnydZVDPkCyUNLyEiQWrkGsGGmukycI+7/7+E6mma/kKRsxYGcYOUiMgYjQTB/wEOu3sFwMzSZtbh7kPJljZ13D0aeVRNQyISoEauETwEzKqZnwV8L5lymuPg4TKlSpUeNQ2JSIAaCYJ2dy+MzMTTHcmVNPVGO5PpjEBEAtRIEAya2YUjM2Z2EXAouZKmnoaXEJGQNXKN4H3AV8xsVzx/KtGjK1uGhpcQkZA10qFsvZmdA5xNNODcL9x9OPHKppBGHhWRkDXy8PobgU533+TuTwJdZvZXyZc2dfKFEmawoENBICLhaeQawV/GTygDwN33AX+ZXElTL18osqAjRyZ9Ih2tRURmtkb2fKnah9KYWRpoqUPn/EBRzUIiEqxGLhZ/B/iyma0lGmpiDfCtRKuaYv16VrGIBKyRM4KbiTqVvQu4EXiCsR3MZrx8oaggEJFgHTMI4gfYPwpsA1YAlwJbGvlwM7vczJ42s61mdssk6/2BmVXM7F83WPdJlR9QEIhIuCZsGjKzs4BrgeuAfuBLAO7+mkY+OL6WcBvwOqKhq9eb2f3u/lSd9f6OqAlqyh0qVRgsVXSNQESCNdkZwS+Ijv6vcvdXuvsngMpxfPZKYKu7b3P3EnAvsKrOeu8G7gP2HMdnnzQjfQh6dUYgIoGaLAjeBPwWeNjMPm1mlxJ1KGvUImB7zfyOeNkoM1sEvBFYO9kHmdlqM9tgZhv6+vqOo4Rj0zhDIhK6CYPA3b/m7tcA5wD/DPxbYKGZ3W5mr2/gs+uFho+b/wfg5pEhriepZZ27r3D3Fb29vQ386cbl43GG9FAaEQlVI0NMDAJ3A3eb2QLgauAW4MFj/OoOYEnN/GJg17h1VgD3xt0UeoArzazs7l9vrPznr3/0jEBBICJhaqQfwSh33wvcEf8cy3pguZktA3YSXXi+ftznLRuZNrPPA9+YyhCAmnGGOtU0JCJhOq4gOB7uXjazm4juBkoDd7r7ZjNbE78/6XWBqZIvlJjdnqE9m252KSIiTZFYEAC4+wPAA+OW1Q0Ad//zJGuZSJ86k4lI4IIfZa2/UKRHfQhEJGDBB0G+oHGGRCRsCgI1DYlI4IIOguFKlf1DwxpeQkSCFnQQ7B3UQ+tFRIIOgr4BPbReRCToIBgdZ0hNQyISsKCDoL+gpiERkaCDIK9xhkREFATt2RSdOQ0vISLhCjoI+gslujvbiEc/FREJUtBB0FcoqllIRIIXdBDkCyV6dceQiAQu8CAo6slkIhK8YIOgWnX2Dpb0rGIRCV6wQbD/0DCVqqsPgYgEL9ggONKrWEEgImELPgg08qiIhC7gIIiGl+jVGYGIBC7cINDIoyIiQMhBUCiSThlzZ2WbXYqISFMFGwTR8BI5UikNLyEiYQs2CPSsYhGRSNhBoHGGRERCDoISPZ26dVREJMggcHedEYiIxIIMgkKxTLFc1bOKRUQINAhGnlWskUdFRAINAj2rWETkiLCDQE1DIiJhBkGfxhkSERkVZBD0x2cE83X7qIhImEGQLxSZ35Elmw5y80VExghyT5gfKGl4CRGRWKJBYGaXm9nTZrbVzG6p8/5bzOyJ+OfHZnZ+kvWM6B8s6oE0IiKxxILAzNLAbcAVwIuA68zsReNW+zXwR+7+EuBDwLqk6qmVL+iMQERkRJJnBCuBre6+zd1LwL3AqtoV3P3H7r4vnn0UWJxgPaPyAxp5VERkRJJBsAjYXjO/I142kXcA36r3hpmtNrMNZrahr6/veRV1eLjCQLFMrzqTiYgAyQZBvSe+eN0VzV5DFAQ313vf3de5+wp3X9Hb2/u8iuofHBleQtcIREQAMgl+9g5gSc38YmDX+JXM7CXAZ4Ar3L0/wXoAPatYRGS8JM8I1gPLzWyZmeWAa4H7a1cws6XAV4F/4+6/TLCWURpnSERkrMTOCNy9bGY3Ad8B0sCd7r7ZzNbE768F/jPQDXzKzADK7r4iqZrgyMijGmdIRCSSZNMQ7v4A8MC4ZWtrpv8C+Iskaxivr6CmIRGRWsH1LM4XinS1ZWjPpptdiojItBBgEJTULCQiUiO4IOgvFOlWs5CIyKjggiBfKOqMQESkRoBBoHGGRERqBRUE5UqVfUMKAhGRWkEFwd6hEu7qQyAiUiuoIMgPjHQm0xmBiMiIsIJAw0uIiBwlqCDoH4yCQCOPiogcEVQQjDYN6YxARGRUWEFQKJLLpJjdlugQSyIiM0pgQVCit6uNeKRTEREhuCAo0q1bR0VExgguCHTrqIjIWAEGgc4IRERqBRME7k5/oaSRR0VExgkmCA4cGqZcdTUNiYiME0wQjPYqVtOQiMgYAQVB1JmsV2cEIiJjBBQE8fASCgIRkTGCCYKXn9HNXW9fydIFHc0uRURkWglmrIXurjYuOau32WWIiEw7wZwRiIhIfQoCEZHAKQhERAKnIBARCZyCQEQkcAoCEZHAKQhERAKnIBARCZyCQEQkcAoCEZHAKQhERAKnIBARCVyiQWBml5vZ02a21cxuqfO+mdmt8ftPmNmFSdYjIiJHSywIzCwN3AZcAbwIuM7MXjRutSuA5fHPauD2pOoREZH6kjwjWAlsdfdt7l4C7gVWjVtnFXCXRx4F5pnZqQnWJCIi4yT5PIJFwPaa+R3AxQ2sswjYXbuSma0mOmMAKJjZ0/F0D5A/WQXPENrmMGibwzCV2/yCid5IMgiszjI/gXVw93XAuqP+gNkGd19xYuXNTNrmMGibwzBdtjnJpqEdwJKa+cXArhNYR0REEpRkEKwHlpvZMjPLAdcC949b537gbfHdQy8DDrj77vEfJCIiyUmsacjdy2Z2E/AdIA3c6e6bzWxN/P5a4AHgSmArMATccJx/5qjmogBom8OgbQ7DtNhmcz+qSV5ERAKinsUiIoFTEIiIBG7GBsGxhq9oRWb2rJk9aWaPm9mGZteTBDO708z2mNmmmmULzOy7ZvZM/Dq/mTWebBNs89+Y2c74u37czK5sZo0nk5ktMbOHzWyLmW02s/fGy1v2e55km6fF9zwjrxHEw1f8Engd0S2o64Hr3P2pphaWMDN7Fljh7i3b6cbMLgEKRD3OXxwv+wiw190/HIf+fHe/uZl1nkwTbPPfAAV3/1gza0tCPHrAqe7+mJnNBjYC/wr4c1r0e55km9/MNPieZ+oZQSPDV8gM5O6PAHvHLV4FfCGe/gLRf6CWMcE2tyx33+3uj8XTA8AWohEFWvZ7nmSbp4WZGgQTDU3R6hx40Mw2xsNuhGLhSP+S+PWUJtczVW6KR+W9s5WaSWqZ2enAS4GfEsj3PG6bYRp8zzM1CBoamqIFvcLdLyQatfXGuElBWtPtwJnABURjb/19c8s5+cysC7gPeJ+7H2x2PVOhzjZPi+95pgZBkENTuPuu+HUP8DWiJrIQ/G5kVNr4dU+T60mcu//O3SvuXgU+TYt912aWJdoh3u3uX40Xt/T3XG+bp8v3PFODoJHhK1qKmXXGF5kws07g9cCmyX+rZdwP/Fk8/WfAPzWxlikxbjj2N9JC37WZGfBZYIu7/8+at1r2e55om6fL9zwj7xoCiG+z+geODF/x35pcUqLM7AyiswCIhgb53624zWZ2D/BqouF5fwd8EPg68GVgKfAccLW7t8zF1Qm2+dVEzQUOPAu8s1XG4TKzVwI/BJ4EqvHi/0DUZt6S3/Mk23wd0+B7nrFBICIiJ8dMbRoSEZGTREEgIhI4BYGISOAUBCIigVMQiIgETkEgEjOzSs0okI+fzFFtzez02tFFRaaTxB5VKTIDHXL3C5pdhMhU0xmByDHEz4H4OzP7l/jn9+PlLzCzh+IBwx4ys6Xx8oVm9jUz+3n884fxR6XN7NPxePQPmtmseP33mNlT8efc26TNlIApCESOmDWuaeiamvcOuvtK4JNEPdqJp+9y95cAdwO3xstvBX7g7ucDFwKb4+XLgdvc/VxgP/CmePktwEvjz1mT1MaJTEQ9i0ViZlZw9646y58FXuvu2+KBw37r7t1mlid62MhwvHy3u/eYWR+w2N2LNZ9xOvBdd18ez98MZN39v5rZt4keTPN14OvuXkh4U0XG0BmBSGN8gumJ1qmnWDNd4cg1ujcAtwEXARvNTNfuZEopCEQac03N60/i6R8TjXwL8BbgR/H0Q8C7IHqsqpnNmehDzSwFLHH3h4G/BuYBR52ViCRJRx4iR8wys8dr5r/t7iO3kLaZ2U+JDp6ui5e9B7jTzD4A9AE3xMvfC6wzs3cQHfm/i+ihI/WkgS+a2VyiBy593N33n7QtEmmArhGIHEN8jWCFu+ebXYtIEtQ0JCISOJ0RiIgETmcEIiKBUxCIiAROQSAiEjgFgYhI4BQEIiKB+/9TztNLK4p5SQAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"We are getting close to 90% accuracy with logistic regression, which is fine considering the low effort."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}